{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fluid-israeli",
   "metadata": {},
   "source": [
    "# Machine Learning & Application \n",
    "## 2021-2022\n",
    "# Projet Rubik's cube\n",
    "\n",
    "Le projet porte sur un problème d'apprentissage supervisé. Le problème fait parti des données du [challenge des données](https://challengedata.ens.fr/challenges/20) ENS et s'intitule \"Solve 2x2x2 Rubik's cube\" et est présenté par la société LumenAI. Une [vidéo](https://www.college-de-france.fr/video/stephane-mallat/2019) décrivant le problème se trouve sur le site du collège de France. \n",
    "\n",
    "Les autres challenges sont aussi très intéressants, mais nécessitent plus de connaissances en machine learning (par exemple de l'apprentissage sur des séries temporelles, sur des images, des sons, du texte, etc...). D'où le choix de ce challenge dont les données sont très proches de problèmes sur lesquels on a travaillé.\n",
    "\n",
    "La résolution d'un rubik's cube peut être vu comme un problème d'intelligence artificielle (par exemple en utilisant des techniques de recherches avec des heuristiques). On peut même étudier le graphe du jeu du point de vue de la théorie des graphes et découvrir qu'en fait il existe toujours un chemin relativement court à une solution.\n",
    "\n",
    "ex dans la litérature:\n",
    " * \"The Diameter of the Rubik's Cube Group Is Twenty\", *T. Rokicki, H. Kociemba, M. Davidson, and J. Dethridge*, SIAM Review, 2014, Vol. 56, No. 4 : pp. 645-670.\n",
    " * \"Solving Rubik’s Cube Using Graph Theory\", Khemani C., Doshi J., Duseja J., Shah K., Udmale S., Sambhe V. (2019) in: Verma N., Ghosh A. (eds) Computational Intelligence: Theories, Applications and Future Directions - Volume I. Advances in Intelligent Systems and Computing, vol 798. Springer, Singapore. \n",
    " \n",
    "Ici, on a une base de données qui contient la description de rubik's cubes ainsi que le nombre minimal de coups pour les résoudre. On ne sait pas comment les problèmes ont été générés (est-ce que cela entraine un biais dans les problèmes, est-ce que plusieurs problèmes similaires sont présents dans la base? (Ici par similaire, on pourrait peut être avoir deux problèmes qui apparaissent dans la base, mais en permuttant certaines couleurs, on aurait peut-être exactement le même problème!)). Cependant, on vous demande de constuire un modèle pour nous aider à prédire le nombre de coups minimal pour un problème donné. Ensuite, vous pourriez utiliser ce modèle dans un algorithme de recherche étudié en cours d'IA.\n",
    "\n",
    "On peut le voir comme un problème de regression où il faut deviner le nombre minimal de coups pour résoudre le rubik's cube, ou comme un problème de classification où la classe d'un état du rubik's cube est le nombre minimal de coups pour le résoudre (donc on pourrait avoir au plus 19 classes). Toutes les méthodes que l'on a vu en cours peuvent s'appliquer.\n",
    "\n",
    "## Les données et le site du challenge\n",
    "\n",
    "Le projet s'effectue en binôme. Vous devez ouvrir un compte pour le binôme sur le site du challenge, choisissez de participer seul (le binôme sera un seul participant au challenge), puis inscrivez-vous au challenge du cours *M1 MIAGE Dauphine - PSL - 2021-2022*.\n",
    "\n",
    "Vous aurez accès à trois ensembles: \n",
    " * `x_train` qui contient la description de 1.837.079 différents rubik's cubes. Chaque rubik's cube est représenté par 25 attributs (lisez la description sur le site du challenge).\n",
    " * `y_train` qui contient le nombre minimal de coups pour résoudre chacun des 1.837.079 différents rubik's cubes. \n",
    " Ces données sont vos données d'entrainement.\n",
    " * enfin `x_test` qui contient la description de 1.837.080 nouveaux rubik's cubes. Vous ne connaissez pas le nombre minimal pour chacun de ces problèmes. \n",
    " \n",
    "Pour participer aux challenges, il vous faudra uploader sur le site votre prédiction sur les rubik's cubes du fichier `x_test` et le site du challenge vous donnera un score. Pour ce score, le site utilise l'erreur moyenne absolue: pour les n=1.837.080 exemples du fichier test, on fait la moyenne entre le vrai nombre minimal de coups $y_i$ et votre prédiction $z_i$: \n",
    "$$ \\frac{\\sum_{i=1}^n |y_i-z_i|}{n}$$\n",
    "\n",
    "Malheureusement (pour vous), le site ne vous donnera pas plus d'information que votre score, vous ne pourrez pas savoir quelles sont vos bonnes prédictions et quelles sont vos erreurs. Pire, le site vous permettra d'uploader une prédiction que deux fois par jour!\n",
    "\n",
    "\n",
    "## Soumission et rapport\n",
    "\n",
    "Un des membres du binôme devra remplir le formulaire Forms de l'équipe Teams du cours (onglet General) pour enregistrer les membres du binôme et le login du binôme qui utilisé sur le site du challenge ENS. \n",
    "Avant le **jeudi 2 décembre à 12h** vous devez 1) avoir créé votre compte pour le binôme et inscrit le binôme sur le challenge du cours, et 2) rempli les informations sur le formulaire Forms.\n",
    "\n",
    "Vous pouvez faire le projet seul, mais vous serez évalué comme un binôme. Si vous tenez absolument à former un trinôme, contactez moi par email, mais sachez que dès lors, les attentes seront plus élevées.\n",
    "\n",
    "Le deadline pour le projet sera le **dimanche 20 décembre 23:59**\n",
    "Vous devrez à ce moment là avoir fait trois choses:\n",
    "* avoir rendu un rapport\n",
    "* avoir rendu un notebook jupyter ou collab contenant le code pour générer votre solution\n",
    "* avoir soumis une solution sur le site du challenge ENS.\n",
    "\n",
    "Le notebook et le rapport seront à soumettre sous myCourse.\n",
    "\n",
    "Le rapport est un document **pdf** et devra être un document structuré qui explique vos choix, explique votre solution et donne votre résultat. Ne présentez ni le cours, ni le contexte, seul votre travail est important. Le rapport est de **6** pages maximum au format A4 (sans utiliser une taille de police inférieure strictement à 12). Vous pouvez ajouter une annexe à ce rapport (au format pdf ou sous la forme d'un notebook jupyter), étant entendu que le lecteur n'est pas obligé de lire l'annexe. Votre mission est de proposer un modèle de prédiction pour ce problème, votre rapport doit justifier comment vous avez répondu (complètement ou pas) à cette mission (par exemple, vous pouvez décrire ce que vous avez essayé, ce qui a marché ou non, pourquoi vous avez essayé autre chose...). Une autre façon de décrire ce qu'on attend du rapport est la suivante: votre manageur a donné à plusieurs équipes la même tâche d'apprentissage supervisé. Vous devez lui présenter dans ce rapport des arguments qui justifient la qualité de votre approche et de vos résultats. Votre manageur connait le problème, mais n'est pas forcément un expert du domaine. A vous de le convaincre d'utiliser votre solution! (attention, si vous connaissez aussi les limitations de votre solution, il est bon de les exposer aussi!). \n",
    "\n",
    "L'évaluation portera sur la qualité de votre analyse, même si vos résultats sont peu concluant. Pour caricaturer, un modèle qui gagnerait la compétition sans pouvoir expliquer ce qu'il a fait n'aura pas une bonne note pour le projet du cours (mais bravo, il a gagné la compétition!). Autre caricature, un projet qui applique un seul algorithme et conclue que ça ne fonctionne pas bien n'aura pas non plus une bonne note.\n",
    "\n",
    "Une soutenance sera organisée lors de la première semaine de cours (après les examens). Elle permettra de compléter l'évaluation et de vous donner un retour sur votre travail.\n",
    " Cette soutenance ne demande aucune préparation de votre part. Elle durera une douzaine/quinzaine de minutes par groupe. La soutenance consistera en un échange au sujet de vos résultats et votre rapport. Si la soutenance fait apparaître qu’un des membres n’a pas beaucoup contribué, sa note pourra être revue à la baisse. Egalement, on pourra vous demander de montrer le code et de fournir les résultats que vous avez obtenu lors des exercices d’implémentation des TDs.\n",
    "\n",
    "Ce projet compte pour 40% de la note de l’UE. Il est donc souhaitable que la note corresponde au travail de votre groupe, et non aux conseils d’autres groupes, d’autres étudiants ou d’internet. Si vous utilisez des sources (articles de recherche, posts sur internet, etc...), vous devez mentionnez vos sources dans le rapport (sinon, cela s'appelle du plagiat, et cela peut être puni par un conseil de discipline).\n",
    "\n",
    "Les quelques lignes de code ci-dessous lisent simplement les fichiers sources et affiche la taille des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acknowledged-battery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille des données:\n",
      "X: (1837079, 24)\n",
      "Y: (1837079,)\n",
      "test: (1837080, 24)\n",
      "[[4 1 1 ... 3 5 3]\n",
      " [6 5 2 ... 3 5 2]\n",
      " [5 3 3 ... 2 5 2]\n",
      " ...\n",
      " [3 3 3 ... 1 5 2]\n",
      " [5 3 5 ... 6 5 1]\n",
      " [3 1 4 ... 2 5 3]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "df_train_in = pd.read_csv(\"train_input.csv\")\n",
    "df_train_in = df_train_in.drop(columns=['ID'])\n",
    "X_complet = df_train_in.to_numpy()\n",
    "df_train_out = pd.read_csv(\"train_output.csv\")\n",
    "df_train_out = df_train_out.distance\n",
    "y_complet = df_train_out.to_numpy()\n",
    "print(\"taille des données:\")\n",
    "print(\"X:\", X_complet.shape)\n",
    "print(\"Y:\", y_complet.shape)\n",
    "\n",
    "df_test_in = pd.read_csv(\"test_input.csv\")\n",
    "df_test_in = df_test_in.drop(columns=['ID'])\n",
    "Xtest = df_test_in.to_numpy()\n",
    "print(\"test:\", Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facial-grounds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moyenne coups : 10.66639050362015\n",
      "mediane coups : 11.0\n",
      "Nom des classes:  [11  9 12 10  8  7 13  6  5  3  4  2 14  1]\n",
      "Nombre d'observations dans chacune des classes:\n",
      " 1          3\n",
      "2         13\n",
      "3         60\n",
      "4        267\n",
      "5       1128\n",
      "6       4485\n",
      "7      16529\n",
      "8      57074\n",
      "9     180254\n",
      "10    465294\n",
      "11    675426\n",
      "12    391268\n",
      "13     45140\n",
      "14       138\n",
      "Name: distance, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"moyenne coups : \" + str(df_train_out.distance.mean()))\n",
    "print(\"mediane coups : \" + str(df_train_out.distance.median()))\n",
    "print(\"Nom des classes: \", df_train_out.distance.unique())\n",
    "print(\"Nombre d'observations dans chacune des classes:\\n\", df_train_out.distance.value_counts().sort_index(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c28de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_train_in.iterrows():\n",
    "    for idx2, row2 in df_train_in.iterrows():\n",
    "        if(idx2 < idx):\n",
    "            pass\n",
    "            #on a déjà comparé ces deux lignes entre elles\n",
    "        else:\n",
    "            tab = [0, 0, 0, 0, 0, 0]\n",
    "            if(idx != idx2):\n",
    "                i = 1\n",
    "                while(i<25):\n",
    "                    if(tab[row[i]-1] == 0):\n",
    "                        tab[row[i]-1] = row2[i]\n",
    "                    elif(tab[row[i]-1] != row2[i]):\n",
    "                        i = 30\n",
    "                    i += 1\n",
    "                if(i < 30):\n",
    "                    print(row)\n",
    "                    print(y[idx][1])\n",
    "                    print(row2)\n",
    "                    print(y[idx2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2141dd40",
   "metadata": {},
   "source": [
    "Notre première approche a été de tester le score que l'on aurait en faisant de l'aléatoire.\n",
    "Comme les données d'entraînement et de test ont le même nombre (à 1 près) de données, on a repris les mêmes proportions des différentes distances dans les données d'entraînement (on les a affiché plus haut) et on les a remises aléatoirement sur les données de test\n",
    "Cette première approche nous a permis d'obtenir un score de 1.2475036470921244 sur le site du challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23ecbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_distance = df_train_out.distance.tolist()\n",
    "rdm_distance.append(11) #dans le jeu de test il y a une ligne en plus, on a donc rajouté 11 comme distance car c'est la distance majoritaire\n",
    "random.shuffle(rdm_distance)\n",
    "df_test_out_rdm = pd.DataFrame(list(zip(df_test_in.ID.tolist(), rdm_distance)), columns=['ID', 'distance'])\n",
    "df_test_out_rdm.to_csv('test_output_random.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba131e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16529\n",
      "57074\n",
      "180254\n",
      "465294\n",
      "675426\n",
      "391268\n",
      "45140\n"
     ]
    }
   ],
   "source": [
    "#sélection d'une partie des données\n",
    "#sélection de 50% des données pour les classes les plus volumineuses\n",
    "\n",
    "df_train_out_sep = [[]for i in range(7)]\n",
    "\n",
    "for idx, valeur in df_train_out.iterrows():\n",
    "    if(valeur.distance == 7):\n",
    "        df_train_out_sep[0].append(idx)\n",
    "    elif(valeur.distance == 8):\n",
    "        df_train_out_sep[1].append(idx)\n",
    "    elif(valeur.distance == 9):\n",
    "        df_train_out_sep[2].append(idx)\n",
    "    elif(valeur.distance == 10):\n",
    "        df_train_out_sep[3].append(idx)\n",
    "    elif(valeur.distance == 11):\n",
    "        df_train_out_sep[4].append(idx)\n",
    "    elif(valeur.distance == 12):\n",
    "        df_train_out_sep[5].append(idx)\n",
    "    elif(valeur.distance == 13):\n",
    "        df_train_out_sep[6].append(idx)\n",
    "\n",
    "for i in range(len(df_train_out_sep)):\n",
    "    print(len(df_train_out_sep[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "135b8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_train_out_sep)): #mélanger avant d'en garder seulement 50%\n",
    "    random.shuffle(df_train_out_sep[i])\n",
    "\n",
    "df_train_out_sep2 = []\n",
    "for i in range(len(df_train_out_sep)):\n",
    "    for j in range(round(len(df_train_out_sep[i])/2)): #on prend que 50% de la base de données\n",
    "        df_train_out_sep2.append(df_train_out.loc[df_train_out_sep[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afe8ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, valeur in df_train_out.iterrows():\n",
    "    if(valeur.distance < 7 or valeur.distance == 14):\n",
    "        df_train_out_sep2.append(valeur)\n",
    "\n",
    "df_train_out_sep3 = pd.DataFrame(data=df_train_out_sep2,index=[i for i in range(len(df_train_out_sep2))], columns=['ID','distance'])\n",
    "df_train_out_reduit = df_train_out_sep3.sample(frac=1).reset_index(drop=True) #melanger pour que ce ne soit pas trié par classe\n",
    "df_train_out_reduit.to_csv('train_out_reduit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fe29889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ID  pos0  pos1  pos2  pos3  pos4  pos5  pos6  pos7  pos8  ...  \\\n",
      "0       1426607     4     3     2     6     1     6     6     6     1  ...   \n",
      "1        293988     5     3     5     1     5     6     6     1     6  ...   \n",
      "2        648371     3     4     1     3     2     2     6     3     4  ...   \n",
      "3       1405346     1     3     2     6     5     5     6     4     4  ...   \n",
      "4          2611     3     1     5     5     2     4     6     6     2  ...   \n",
      "...         ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
      "921581   373362     5     3     1     2     2     6     6     1     2  ...   \n",
      "921582  1708502     3     2     6     4     2     3     6     5     1  ...   \n",
      "921583   648328     3     1     1     4     5     2     6     6     1  ...   \n",
      "921584  1622948     1     4     4     5     1     2     6     2     2  ...   \n",
      "921585  1819791     1     6     4     6     4     1     6     5     2  ...   \n",
      "\n",
      "        pos14  pos15  pos16  pos17  pos18  pos19  pos20  pos21  pos22  pos23  \n",
      "0           4      2      3      1      5      3      4      3      5      5  \n",
      "1           4      3      2      2      4      2      2      3      5      4  \n",
      "2           4      1      1      5      2      6      6      6      5      2  \n",
      "3           4      3      5      4      1      3      6      1      5      6  \n",
      "4           4      4      6      2      4      6      1      3      5      3  \n",
      "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "921581      4      4      1      6      4      3      6      5      5      5  \n",
      "921582      4      1      2      1      2      3      5      1      5      4  \n",
      "921583      4      3      2      5      4      3      1      6      5      2  \n",
      "921584      4      6      3      3      1      2      3      6      5      3  \n",
      "921585      4      2      3      3      1      2      3      4      5      1  \n",
      "\n",
      "[921586 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "df_train_out_reduit = pd.read_csv(\"train_out_reduit.csv\")\n",
    "train_in_red = []\n",
    "for idx, valeur in df_train_out_reduit.iterrows():\n",
    "    train_in_red.append(df_train_in.loc[valeur.ID])\n",
    "\n",
    "df_train_in_reduit = pd.DataFrame(data=train_in_red,index=[i for i in range(len(df_train_out_reduit))], columns=['ID','pos0','pos1','pos2','pos3','pos4','pos5','pos6','pos7','pos8','pos9','pos10','pos11','pos12','pos13','pos14','pos15','pos16','pos17','pos18','pos19','pos20','pos21','pos22','pos23'])\n",
    "df_train_in_reduit.to_csv('train_in_reduit.csv', index=False)\n",
    "print(df_train_in_reduit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56eb211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  nom\n",
      "0   0    1\n",
      "1   1    1\n",
      "2   2    2\n",
      "3   3    3\n",
      "4   4    4\n",
      "5   5    3\n",
      "6   6    2\n",
      "7   7    1\n",
      "8   8    1\n",
      "   id  pos1  pos2\n",
      "0   0     1     4\n",
      "1   1     1     5\n",
      "2   2     2     7\n",
      "3   3     3     3\n",
      "4   4     4     5\n",
      "5   5     3     7\n",
      "6   6     2     0\n",
      "7   7     1     2\n",
      "8   8     1     9\n",
      "   id  nom\n",
      "0   3    3\n",
      "1   2    2\n",
      "2   4    4\n",
      "3   1    1\n",
      "4   0    1\n",
      "5   5    3\n",
      "   id  pos1  pos2\n",
      "0   3     3     3\n",
      "1   2     2     7\n",
      "2   4     4     5\n",
      "3   1     1     5\n",
      "4   0     1     4\n",
      "5   5     3     7\n"
     ]
    }
   ],
   "source": [
    "#visualisation code précédent\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "test = [[]for i in range(2)]\n",
    "test2 = np.array([[0,1], [1,1], [2,2], [3,3], [4,4], [5,3], [6,2], [7,1], [8,1]])\n",
    "inp = np.array([[0,1,4], [1,1,5], [2,2,7], [3,3,3], [4,4,5], [5,3,7], [6,2,0], [7,1,2], [8,1,9]])\n",
    "test3 = pd.DataFrame(data=test2,columns=['id','nom'])\n",
    "inp2 = pd.DataFrame(data=inp,columns=['id','pos1', 'pos2'])\n",
    "\n",
    "print(test3)\n",
    "print(inp2)\n",
    "\n",
    "for idx, valeur in test3.iterrows():\n",
    "    if(valeur.nom == 1):\n",
    "        test[0].append(idx)\n",
    "    elif(valeur.nom == 2):\n",
    "        test[1].append(idx)\n",
    "\n",
    "\n",
    "for i in range(len(test)):\n",
    "    random.shuffle(test[i])\n",
    "\n",
    "test4 = []\n",
    "for i in range(len(test)):\n",
    "    for j in range(round(len(test[i])/2)): #on prend que 50% de la base de données\n",
    "        test4.append(test3.loc[test[i][j]])\n",
    "\n",
    "for idx, valeur in test3.iterrows():\n",
    "    if(valeur.nom > 2):\n",
    "        test4.append(valeur)\n",
    "    \n",
    "test5 = pd.DataFrame(data=test4,index=[i for i in range(len(test4))], columns=['id','nom'])\n",
    "test5_shuffled=test5.sample(frac=1).reset_index(drop=True)\n",
    "print(test5_shuffled)\n",
    "\n",
    "inp3 = []\n",
    "for idx, valeur in test5_shuffled.iterrows():\n",
    "    inp3.append(inp2.loc[valeur.id])\n",
    "\n",
    "inp4 = pd.DataFrame(data=inp3,index=[i for i in range(len(test5_shuffled))], columns=['id','pos1', 'pos2'])\n",
    "print(inp4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_train_in_reduit = pd.read_csv(\"train_in_reduit.csv\")\n",
    "X_reduit = df_train_in_reduit.to_numpy()\n",
    "df_train_out_reduit = pd.read_csv(\"train_out_reduit.csv\")\n",
    "y_reduit = df_train_out_reduit.to_numpy()\n",
    "\n",
    "df_test_in = pd.read_csv(\"test_input.csv\")\n",
    "Xtest = df_test_in.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131259b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_reduit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18904/1101170268.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkppv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mkppv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_reduit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_reduit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkppv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_reduit' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "kppv = KNeighborsClassifier(n_neighbors=5,  metric='euclidean')\n",
    "kppv.fit(X_reduit, y_reduit)\n",
    "out = kppv.predict(Xtest)\n",
    "print(out)\n",
    "\n",
    "df_test_out_reduit_kppv = pd.DataFrame(list(zip(df_test_in.ID.tolist(), out)), columns=['ID', 'distance'])\n",
    "df_test_out_reduit_kppv.to_csv('test_out_reduit_kppv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dea6f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 11 11 ... 11 11 11]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,40,30,24), max_iter=1000)\n",
    "mlp.fit(X_complet, y_complet)\n",
    "out = mlp.predict(Xtest)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36cddcd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13576/1918018322.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_complet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_complet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \"\"\"\n\u001b[0;32m    389\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    346\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    349\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mC\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse\u001b[0m \u001b[1;34m'np.ascontiguousarray'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \"\"\"\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"arpack\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"randomized\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m             raise ValueError(\n",
      "\u001b[1;32mc:\\users\\magny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit_truncated\u001b[1;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0msvd_solver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"randomized\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m             \u001b[1;31m# sign flipping is done inside\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m             U, S, Vt = randomized_svd(\n\u001b[0m\u001b[0;32m    581\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m                 \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m     Q = randomized_range_finder(\n\u001b[0m\u001b[0;32m    396\u001b[0m         \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_random\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"LU\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"QR\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\linalg\\decomp_lu.py\u001b[0m in \u001b[0;36mlu\u001b[1;34m(a, permute_l, overwrite_a, check_finite)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[0moverwrite_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moverwrite_a\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_datacopied\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[0mflu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_flinalg_funcs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m     \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpermute_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverwrite_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         raise ValueError('illegal value in %dth argument of '\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), PCA(n_components=10), MLPClassifier(hidden_layer_sizes=(50,50,50), max_iter=1000))\n",
    "pipe.fit(X_complet, y_complet)\n",
    "out = pipe.predict(Xtest)\n",
    "print(out)\n",
    "#que des 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dd2912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1837079, 24)\n",
      "[0.05471462 0.05466935 0.05463578 0.05460393 0.054564   0.05453374\n",
      " 0.05452152 0.05448935 0.05446838 0.0544395  0.05441882 0.05434077\n",
      " 0.05433003 0.05430834 0.03956166 0.03952672 0.03951675 0.03948719\n",
      " 0.03946538 0.03940416]\n",
      "[[4 1 1 ... 3 5 3]\n",
      " [6 5 2 ... 3 5 2]\n",
      " [5 3 3 ... 2 5 2]\n",
      " ...\n",
      " [3 3 3 ... 1 5 2]\n",
      " [5 3 5 ... 6 5 1]\n",
      " [3 1 4 ... 2 5 3]]\n",
      "X: (1837079, 24)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"X:\", X_complet.shape)\n",
    "pca = PCA(n_components='mle')\n",
    "pca.fit_transform(X_complet)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(X_complet)\n",
    "print(\"X:\", X_complet.shape)\n",
    "#out = pca.predict(Xtest)\n",
    "#print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265e74da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moyenne coups : 11.000090905132057\n",
      "Nom des classes:  [11 12 13]\n",
      "Nombre d'observations dans chacune des classes:\n",
      " [      0       0       0       0       0       0       0       0       0\n",
      "       0       0 1836926     141      13]\n"
     ]
    }
   ],
   "source": [
    "print(\"moyenne coups : \" + str(out.mean()))\n",
    "print(\"Nom des classes: \", np.unique(out))\n",
    "print(\"Nombre d'observations dans chacune des classes:\\n\", np.bincount(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4cb04d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_in = pd.read_csv(\"test_input.csv\")\n",
    "df_test_out_neural = pd.DataFrame(list(zip(df_test_in.ID.tolist(), out)), columns=['ID', 'distance'])\n",
    "df_test_out_neural.to_csv('test_output_neural_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
